{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2387d51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\gruhi\\anaconda3\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\gruhi\\anaconda3\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\gruhi\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\gruhi\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\gruhi\\anaconda3\\lib\\site-packages (1.8.2.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\gruhi\\anaconda3\\lib\\site-packages (from wordcloud) (1.21.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\gruhi\\anaconda3\\lib\\site-packages (from wordcloud) (3.5.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\gruhi\\anaconda3\\lib\\site-packages (from wordcloud) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\gruhi\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\gruhi\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gruhi\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\gruhi\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\gruhi\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\gruhi\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gruhi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gruhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\gruhi\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\gruhi\\anaconda3\\lib\\site-packages (from imblearn) (0.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\gruhi\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\gruhi\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\gruhi\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.9.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\gruhi\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.0.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\gruhi\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gruhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "#import pandas_profiling\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.feature_extraction.text as text\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import warnings\n",
    "!pip install contractions\n",
    "import contractions\n",
    "from collections import Counter\n",
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "from sklearn. preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "import gensim\n",
    "!pip install imblearn\n",
    "from imblearn.pipeline import make_pipeline\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da39616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c265950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('data.json',lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee49b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['U.S. NEWS', 'COMEDY', 'PARENTING', 'WORLD NEWS', 'CULTURE & ARTS',\n",
       "       'TECH', 'SPORTS', 'ENTERTAINMENT', 'POLITICS', 'WEIRD NEWS',\n",
       "       'ENVIRONMENT', 'EDUCATION', 'CRIME', 'SCIENCE', 'WELLNESS',\n",
       "       'BUSINESS', 'STYLE & BEAUTY', 'FOOD & DRINK', 'MEDIA',\n",
       "       'QUEER VOICES', 'HOME & LIVING', 'WOMEN', 'BLACK VOICES', 'TRAVEL',\n",
       "       'MONEY', 'RELIGION', 'LATINO VOICES', 'IMPACT', 'WEDDINGS',\n",
       "       'COLLEGE', 'PARENTS', 'ARTS & CULTURE', 'STYLE', 'GREEN', 'TASTE',\n",
       "       'HEALTHY LIVING', 'THE WORLDPOST', 'GOOD NEWS', 'WORLDPOST',\n",
       "       'FIFTY', 'ARTS', 'DIVORCE'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12bcbc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeab7c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['headline', 'short_description'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9f83ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209038, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e2f3078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d95d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da7c736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['all_words'] = df['headline']+\" \"+df['short_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f8e7486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters Health experts said it is too early to predict whether demand would match up with the 171 million doses of the new boosters the U.S. ordered for the fall.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['all_words'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd03b72",
   "metadata": {},
   "source": [
    "# data Cleaning / preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47754287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(combined_words):\n",
    "    expanded_words = []   \n",
    "    for word in combined_words.split():\n",
    "        expanded_words.append(contractions.fix(word))  \n",
    "    expanded_text = ' '.join(expanded_words)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11876df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27905ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(x):\n",
    "    lst = [i for i in x.split() if not i in stop_words]\n",
    "    data = ' '.join(lst)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa6de2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    data = str(data)\n",
    "    data = data.lower()\n",
    "    data = expand_contractions(data)\n",
    "    data = re.sub('@[A-Za-z0-9_]+',' ', data)\n",
    "    data = re.sub('#[A-Za-z0-9_]+',' ', data)\n",
    "    data = re.sub('(http\\S+)', ' ', data)\n",
    "    data = re.sub('www.\\S+', ' ', data)\n",
    "    data = re.sub('[()!?]', ' ', data)\n",
    "    data = re.sub('\\[.*?\\]',' ', data)\n",
    "    data = re.sub('[^a-z0-9]',' ', data)\n",
    "    data = re.sub(' +',' ', data)\n",
    "    data = remove_stop_words(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "860b4dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = df.loc[:,['all_words','category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e3303de",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['clean_all_words'] = clean_df['all_words'].apply(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cb58ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_words</th>\n",
       "      <th>category</th>\n",
       "      <th>clean_all_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>4 million americans roll sleeves omicron targe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>american airlines flyer charged banned life pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>23 funniest tweets cats dogs week sept 17 23 d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>funniest tweets parents week sept 17 23 accide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>woman called cops black bird watcher loses law...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           all_words   category  \\\n",
       "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
       "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
       "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
       "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
       "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
       "\n",
       "                                     clean_all_words  \n",
       "0  4 million americans roll sleeves omicron targe...  \n",
       "1  american airlines flyer charged banned life pu...  \n",
       "2  23 funniest tweets cats dogs week sept 17 23 d...  \n",
       "3  funniest tweets parents week sept 17 23 accide...  \n",
       "4  woman called cops black bird watcher loses law...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e11c3639",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clean_df['clean_all_words'].tolist()\n",
    "label = LabelEncoder()\n",
    "Y = label.fit_transform(clean_df['category']).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0725a3",
   "metadata": {},
   "source": [
    "# Test Train With SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d92eccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test, y_train,y_test = train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b45521ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train_ex = vectorizer.fit_transform(x_train)\n",
    "x_test_ex = vectorizer.transform(x_test)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "x_train_smote, y_train_smote = smote.fit_resample(x_train_ex, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b50772c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1123458, 77454)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_smote.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7b6c7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156778, 77454)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_ex.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6217af88",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7073677a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5530424799081516\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.23702   0.28378   0.25830       370\n",
      "           1    0.22611   0.30990   0.26146       313\n",
      "           2    0.42105   0.44991   0.43500      1138\n",
      "           3    0.47941   0.43467   0.45594      1500\n",
      "           4    0.38133   0.45981   0.41691       311\n",
      "           5    0.52332   0.41679   0.46402      1346\n",
      "           6    0.44774   0.68326   0.54098       884\n",
      "           7    0.25672   0.32950   0.28859       261\n",
      "           8    0.59535   0.73818   0.65911       867\n",
      "           9    0.26471   0.54435   0.35620       248\n",
      "          10    0.74155   0.57111   0.64526      4416\n",
      "          11    0.32272   0.39583   0.35556       384\n",
      "          12    0.18038   0.33784   0.23518       370\n",
      "          13    0.63221   0.63659   0.63439      1585\n",
      "          14    0.24308   0.37386   0.29461       329\n",
      "          15    0.33373   0.43662   0.37831       639\n",
      "          16    0.46698   0.11633   0.18627      1702\n",
      "          17    0.68854   0.71093   0.69956      1107\n",
      "          18    0.27651   0.43457   0.33797       810\n",
      "          19    0.33827   0.48070   0.39710       285\n",
      "          20    0.38238   0.59547   0.46571       707\n",
      "          21    0.32250   0.59447   0.41815       434\n",
      "          22    0.48103   0.49359   0.48723      2261\n",
      "          23    0.26248   0.34386   0.29771       887\n",
      "          24    0.86473   0.60774   0.71381      8836\n",
      "          25    0.69237   0.59281   0.63873      1530\n",
      "          26    0.47156   0.53754   0.50239       586\n",
      "          27    0.54664   0.56706   0.55667       589\n",
      "          28    0.70126   0.74371   0.72186      1272\n",
      "          29    0.34360   0.36678   0.35481       578\n",
      "          30    0.74441   0.73168   0.73799      2456\n",
      "          31    0.27364   0.38508   0.31993       496\n",
      "          32    0.36984   0.51793   0.43154       502\n",
      "          33    0.41978   0.49724   0.45524       905\n",
      "          34    0.74158   0.70905   0.72495      2485\n",
      "          35    0.17066   0.33242   0.22554       364\n",
      "          36    0.66806   0.69869   0.68303       916\n",
      "          37    0.35873   0.37106   0.36479       698\n",
      "          38    0.64368   0.69075   0.66638      4553\n",
      "          39    0.28070   0.32614   0.30172       834\n",
      "          40    0.35073   0.40926   0.37774       821\n",
      "          41    0.36190   0.38832   0.37465       685\n",
      "\n",
      "    accuracy                        0.55304     52260\n",
      "   macro avg    0.44069   0.49155   0.45527     52260\n",
      "weighted avg    0.59280   0.55304   0.56146     52260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SMOTE(random_state=42),\n",
    "    MultinomialNB()\n",
    ")\n",
    "pipeline.fit(x_train, y_train)\n",
    "y_pred = pipeline.predict(x_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a65d028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5436471488710295\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.24891   0.30811   0.27536       370\n",
      "           1    0.24934   0.30351   0.27378       313\n",
      "           2    0.39330   0.47452   0.43011      1138\n",
      "           3    0.43462   0.47200   0.45254      1500\n",
      "           4    0.31905   0.43087   0.36662       311\n",
      "           5    0.37296   0.45914   0.41159      1346\n",
      "           6    0.44779   0.53846   0.48896       884\n",
      "           7    0.34653   0.26820   0.30238       261\n",
      "           8    0.67269   0.73010   0.70022       867\n",
      "           9    0.29000   0.46774   0.35802       248\n",
      "          10    0.68314   0.59171   0.63415      4416\n",
      "          11    0.37811   0.39583   0.38677       384\n",
      "          12    0.17724   0.29459   0.22132       370\n",
      "          13    0.63625   0.62902   0.63261      1585\n",
      "          14    0.22439   0.27964   0.24899       329\n",
      "          15    0.28846   0.37559   0.32631       639\n",
      "          16    0.30364   0.33843   0.32009      1702\n",
      "          17    0.71185   0.71635   0.71409      1107\n",
      "          18    0.24364   0.33086   0.28063       810\n",
      "          19    0.43189   0.45614   0.44369       285\n",
      "          20    0.36908   0.58416   0.45235       707\n",
      "          21    0.40182   0.50922   0.44919       434\n",
      "          22    0.54488   0.45378   0.49517      2261\n",
      "          23    0.25231   0.33822   0.28902       887\n",
      "          24    0.83035   0.60435   0.69955      8836\n",
      "          25    0.71163   0.65163   0.68031      1530\n",
      "          26    0.42156   0.52730   0.46854       586\n",
      "          27    0.48571   0.54839   0.51515       589\n",
      "          28    0.66718   0.67767   0.67239      1272\n",
      "          29    0.32073   0.39619   0.35449       578\n",
      "          30    0.79231   0.73005   0.75991      2456\n",
      "          31    0.28192   0.34274   0.30937       496\n",
      "          32    0.41818   0.50398   0.45709       502\n",
      "          33    0.41602   0.47072   0.44168       905\n",
      "          34    0.73370   0.69738   0.71508      2485\n",
      "          35    0.18912   0.20055   0.19467       364\n",
      "          36    0.76382   0.72380   0.74327       916\n",
      "          37    0.30048   0.35817   0.32680       698\n",
      "          38    0.68119   0.58050   0.62682      4553\n",
      "          39    0.24444   0.34293   0.28543       834\n",
      "          40    0.36694   0.42996   0.39596       821\n",
      "          41    0.36977   0.40000   0.38429       685\n",
      "\n",
      "    accuracy                        0.54365     52260\n",
      "   macro avg    0.43850   0.47458   0.45202     52260\n",
      "weighted avg    0.58074   0.54365   0.55647     52260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SMOTE(random_state=42),\n",
    "    LogisticRegression(n_jobs=1, C=1e5)\n",
    ")\n",
    "pipeline.fit(x_train, y_train)\n",
    "y_pred = pipeline.predict(x_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6aa1256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.4666666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.16168   0.29189   0.20809       370\n",
      "           1    0.14470   0.32268   0.19980       313\n",
      "           2    0.37645   0.45518   0.41209      1138\n",
      "           3    0.46799   0.34600   0.39785      1500\n",
      "           4    0.21861   0.62701   0.32419       311\n",
      "           5    0.47855   0.43091   0.45348      1346\n",
      "           6    0.42633   0.61538   0.50370       884\n",
      "           7    0.18739   0.42146   0.25943       261\n",
      "           8    0.59410   0.79008   0.67822       867\n",
      "           9    0.16583   0.66532   0.26549       248\n",
      "          10    0.78217   0.30005   0.43372      4416\n",
      "          11    0.24526   0.47135   0.32264       384\n",
      "          12    0.16438   0.38919   0.23114       370\n",
      "          13    0.62190   0.53754   0.57665      1585\n",
      "          14    0.16692   0.33739   0.22334       329\n",
      "          15    0.28655   0.38341   0.32798       639\n",
      "          16    0.29388   0.08461   0.13139      1702\n",
      "          17    0.51374   0.75971   0.61297      1107\n",
      "          18    0.31793   0.28889   0.30272       810\n",
      "          19    0.19027   0.63158   0.29245       285\n",
      "          20    0.28910   0.72419   0.41324       707\n",
      "          21    0.21219   0.63364   0.31792       434\n",
      "          22    0.56321   0.24237   0.33890      2261\n",
      "          23    0.28095   0.29425   0.28744       887\n",
      "          24    0.90612   0.42927   0.58255      8836\n",
      "          25    0.65714   0.72157   0.68785      1530\n",
      "          26    0.27833   0.67065   0.39339       586\n",
      "          27    0.30636   0.66214   0.41890       589\n",
      "          28    0.53355   0.76887   0.62995      1272\n",
      "          29    0.22683   0.44464   0.30041       578\n",
      "          30    0.74906   0.65147   0.69686      2456\n",
      "          31    0.23259   0.39718   0.29337       496\n",
      "          32    0.27076   0.64940   0.38218       502\n",
      "          33    0.43084   0.41989   0.42529       905\n",
      "          34    0.69489   0.66720   0.68076      2485\n",
      "          35    0.13889   0.27473   0.18450       364\n",
      "          36    0.59665   0.85590   0.70314       916\n",
      "          37    0.28135   0.25072   0.26515       698\n",
      "          38    0.70495   0.42873   0.53319      4553\n",
      "          39    0.26214   0.37530   0.30868       834\n",
      "          40    0.40489   0.36297   0.38279       821\n",
      "          41    0.31483   0.45547   0.37232       685\n",
      "\n",
      "    accuracy                        0.46667     52260\n",
      "   macro avg    0.38429   0.48881   0.39896     52260\n",
      "weighted avg    0.57606   0.46667   0.47867     52260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SMOTE(random_state=42),\n",
    "    SGDClassifier(loss='hinge', penalty='l2')\n",
    ")\n",
    "pipeline.fit(x_train, y_train)\n",
    "y_pred = pipeline.predict(x_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e7bd5e",
   "metadata": {},
   "source": [
    "# Test Train With Random Over Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c48937c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5465939533103712\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.23326   0.27297   0.25156       370\n",
      "           1    0.21739   0.28754   0.24759       313\n",
      "           2    0.42079   0.44112   0.43072      1138\n",
      "           3    0.48232   0.42733   0.45316      1500\n",
      "           4    0.35610   0.46945   0.40499       311\n",
      "           5    0.46510   0.48514   0.47491      1346\n",
      "           6    0.43228   0.69683   0.53356       884\n",
      "           7    0.24699   0.31418   0.27656       261\n",
      "           8    0.56836   0.73356   0.64048       867\n",
      "           9    0.24501   0.54435   0.33792       248\n",
      "          10    0.74977   0.55639   0.63876      4416\n",
      "          11    0.32573   0.40885   0.36259       384\n",
      "          12    0.15838   0.32703   0.21340       370\n",
      "          13    0.62841   0.63912   0.63372      1585\n",
      "          14    0.24468   0.34954   0.28786       329\n",
      "          15    0.33169   0.42097   0.37103       639\n",
      "          16    0.33076   0.25206   0.28610      1702\n",
      "          17    0.68380   0.70912   0.69623      1107\n",
      "          18    0.27673   0.41852   0.33317       810\n",
      "          19    0.33333   0.46316   0.38767       285\n",
      "          20    0.37959   0.59972   0.46491       707\n",
      "          21    0.31734   0.59447   0.41379       434\n",
      "          22    0.51290   0.46617   0.48842      2261\n",
      "          23    0.25830   0.35964   0.30066       887\n",
      "          24    0.87642   0.59314   0.70748      8836\n",
      "          25    0.69109   0.59804   0.64121      1530\n",
      "          26    0.48143   0.55290   0.51469       586\n",
      "          27    0.56053   0.57385   0.56711       589\n",
      "          28    0.69271   0.73192   0.71177      1272\n",
      "          29    0.34154   0.35986   0.35046       578\n",
      "          30    0.74979   0.73697   0.74333      2456\n",
      "          31    0.27233   0.38105   0.31765       496\n",
      "          32    0.37236   0.52590   0.43600       502\n",
      "          33    0.41906   0.50055   0.45619       905\n",
      "          34    0.75478   0.69859   0.72560      2485\n",
      "          35    0.17034   0.31868   0.22201       364\n",
      "          36    0.63905   0.70742   0.67150       916\n",
      "          37    0.36960   0.35530   0.36231       698\n",
      "          38    0.66699   0.61103   0.63778      4553\n",
      "          39    0.28438   0.32974   0.30539       834\n",
      "          40    0.36038   0.41657   0.38644       821\n",
      "          41    0.35471   0.39562   0.37405       685\n",
      "\n",
      "    accuracy                        0.54659     52260\n",
      "   macro avg    0.43468   0.49106   0.45383     52260\n",
      "weighted avg    0.59203   0.54659   0.55981     52260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    RandomOverSampler(random_state=42),\n",
    "    MultinomialNB()\n",
    ")\n",
    "pipeline.fit(x_train, y_train)\n",
    "y_pred = pipeline.predict(x_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31174cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5462686567164179\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.27402   0.20811   0.23656       370\n",
      "           1    0.25267   0.22684   0.23906       313\n",
      "           2    0.40198   0.49912   0.44532      1138\n",
      "           3    0.40090   0.47467   0.43468      1500\n",
      "           4    0.41538   0.34727   0.37828       311\n",
      "           5    0.38561   0.48960   0.43142      1346\n",
      "           6    0.45438   0.55204   0.49847       884\n",
      "           7    0.31439   0.31801   0.31619       261\n",
      "           8    0.67891   0.74625   0.71099       867\n",
      "           9    0.31173   0.40726   0.35315       248\n",
      "          10    0.67674   0.60824   0.64067      4416\n",
      "          11    0.38585   0.31250   0.34532       384\n",
      "          12    0.28165   0.24054   0.25948       370\n",
      "          13    0.62151   0.64543   0.63324      1585\n",
      "          14    0.26603   0.25228   0.25897       329\n",
      "          15    0.32210   0.40376   0.35833       639\n",
      "          16    0.27502   0.33255   0.30106      1702\n",
      "          17    0.73664   0.69738   0.71647      1107\n",
      "          18    0.25943   0.34815   0.29731       810\n",
      "          19    0.50917   0.38947   0.44135       285\n",
      "          20    0.41853   0.55587   0.47752       707\n",
      "          21    0.40515   0.39862   0.40186       434\n",
      "          22    0.51874   0.42857   0.46936      2261\n",
      "          23    0.23766   0.35287   0.28403       887\n",
      "          24    0.81961   0.62528   0.70938      8836\n",
      "          25    0.64209   0.66013   0.65098      1530\n",
      "          26    0.43515   0.53242   0.47889       586\n",
      "          27    0.49327   0.49745   0.49535       589\n",
      "          28    0.65224   0.71069   0.68021      1272\n",
      "          29    0.34060   0.35121   0.34583       578\n",
      "          30    0.77006   0.75814   0.76405      2456\n",
      "          31    0.30725   0.32460   0.31569       496\n",
      "          32    0.39927   0.43825   0.41785       502\n",
      "          33    0.39142   0.47403   0.42879       905\n",
      "          34    0.70809   0.72918   0.71848      2485\n",
      "          35    0.23874   0.14560   0.18089       364\n",
      "          36    0.65029   0.72271   0.68459       916\n",
      "          37    0.32285   0.33811   0.33030       698\n",
      "          38    0.65208   0.57918   0.61347      4553\n",
      "          39    0.21687   0.30216   0.25251       834\n",
      "          40    0.37891   0.44214   0.40809       821\n",
      "          41    0.36490   0.38248   0.37349       685\n",
      "\n",
      "    accuracy                        0.54627     52260\n",
      "   macro avg    0.44257   0.45831   0.44709     52260\n",
      "weighted avg    0.57068   0.54627   0.55407     52260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    RandomOverSampler(random_state=42),\n",
    "    LogisticRegression(n_jobs=1, C=1e5)\n",
    ")\n",
    "pipeline.fit(x_train, y_train)\n",
    "y_pred = pipeline.predict(x_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bffbca14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.4719862227324914\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.18237   0.32432   0.23346       370\n",
      "           1    0.14663   0.31949   0.20101       313\n",
      "           2    0.39953   0.44903   0.42284      1138\n",
      "           3    0.49546   0.32733   0.39422      1500\n",
      "           4    0.21324   0.65273   0.32146       311\n",
      "           5    0.44240   0.48217   0.46143      1346\n",
      "           6    0.42334   0.62783   0.50569       884\n",
      "           7    0.17966   0.40613   0.24912       261\n",
      "           8    0.56675   0.81776   0.66950       867\n",
      "           9    0.16199   0.69758   0.26292       248\n",
      "          10    0.77734   0.31386   0.44717      4416\n",
      "          11    0.23545   0.46354   0.31228       384\n",
      "          12    0.17143   0.40541   0.24096       370\n",
      "          13    0.59786   0.56278   0.57979      1585\n",
      "          14    0.15729   0.33131   0.21331       329\n",
      "          15    0.28781   0.39906   0.33443       639\n",
      "          16    0.33368   0.18625   0.23906      1702\n",
      "          17    0.53740   0.77236   0.63380      1107\n",
      "          18    0.30543   0.25679   0.27901       810\n",
      "          19    0.19200   0.67368   0.29883       285\n",
      "          20    0.30529   0.71853   0.42851       707\n",
      "          21    0.22213   0.62903   0.32832       434\n",
      "          22    0.58783   0.26493   0.36524      2261\n",
      "          23    0.28012   0.30665   0.29279       887\n",
      "          24    0.91294   0.44624   0.59947      8836\n",
      "          25    0.64380   0.73595   0.68679      1530\n",
      "          26    0.27877   0.66553   0.39295       586\n",
      "          27    0.33933   0.67233   0.45103       589\n",
      "          28    0.54815   0.75629   0.63561      1272\n",
      "          29    0.25176   0.43426   0.31873       578\n",
      "          30    0.74689   0.68485   0.71453      2456\n",
      "          31    0.23875   0.38508   0.29475       496\n",
      "          32    0.27471   0.65339   0.38679       502\n",
      "          33    0.43359   0.43646   0.43502       905\n",
      "          34    0.70520   0.66036   0.68204      2485\n",
      "          35    0.15201   0.28022   0.19710       364\n",
      "          36    0.58585   0.86790   0.69952       916\n",
      "          37    0.27332   0.23926   0.25516       698\n",
      "          38    0.72436   0.34746   0.46965      4553\n",
      "          39    0.27207   0.31775   0.29314       834\n",
      "          40    0.41612   0.38977   0.40252       821\n",
      "          41    0.32386   0.46569   0.38204       685\n",
      "\n",
      "    accuracy                        0.47199     52260\n",
      "   macro avg    0.38866   0.49589   0.40505     52260\n",
      "weighted avg    0.58199   0.47199   0.48457     52260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    RandomOverSampler(random_state=42),\n",
    "    SGDClassifier(loss='hinge', penalty='l2')\n",
    ")\n",
    "pipeline.fit(x_train, y_train)\n",
    "y_pred = pipeline.predict(x_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c18dd1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
